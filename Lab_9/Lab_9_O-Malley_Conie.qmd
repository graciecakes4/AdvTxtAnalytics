---
title: "Lab 8"
author: "Conie O'Malley"
date: "`r Sys.Date()`"
format: 
  pdf: 
    titlepage: true
    toc: true
    toc-depth: 2
    code-block-wrap: true
    number-sections: true
execute:
  echo: true
  freeze: false
  error: false
  jupyter: python3
python:
  version: /Users/coniecakes/anaconda3/envs/datascience/bin/python
---

# Pre Lab
```{r libraries}

required_packages <- c("tidyverse", "quanteda", "readtext", "stm", "stminsights", "wordcloud", "gsl", "topicmodels", 
                        "caret", "gutenbergr", "tidytext", "quanteda.textmodels", "tm", "igraph", "ggraph", "widyr", 
                        "jsonlite", "factoextra", "janeaustenr", "cluster", "SnowballC", "proxy", "stringr", "textclean",
                        "dendextend", "ggdendro", "plotly", "reticulate")

```

## Deliverable 1: Get your working directory and paste below:

"/Users/coniecakes/Library/CloudStorage/OneDrive-Personal/001. Documents - Main/023. Programming Tools/R Studio/AdvTxtAnalytics"

# Part 1: Introduction to K-Means Clustering with Synthetic Data

```{r}

texts <- c(
    # Sports articles
    "The football team scored three touchdowns. The quarterback threw perfect passes.",
    "The basketball team won with slam dunks and three-point shots.",
    "The baseball pitcher threw a perfect game with many strikeouts.",
    "The soccer team scored two goals and won the championship.",
    # Technology articles
    "The computer system processed data using advanced algorithms and databases.",
    "The network router managed bandwidth through secure encryption protocols.",
    "The software code executed functions through compiled programming syntax.",
    "The digital platform integrated APIs with cloud computing architecture.",
    # Food articles
    "The professional chef created sauces and prepared gourmet dishes.",
    "The master baker produced artisan breads and pastries daily.",
    "The culinary team seasoned meats and roasted vegetables.",
    "The kitchen staff garnished plates and plated entrees."
)

```

## Add category labels

```{r categories}

categories <- rep(c("Sports", "Technology", "Food"), each = 4)
cat("Initial document count:", {length(texts)})

```

# Create corpus and preprocess text

```{r corpus}

corpus <- tm::Corpus(tm::VectorSource(texts))
cat("Corpus size:", {length(corpus)})

cleanCorpus <- function(corpus) {
    corpus <- tm::tm_map(corpus, tm::content_transformer(tolower))
    corpus <- tm::tm_map(corpus, tm::removePunctuation)
    corpus <- tm::tm_map(corpus, tm::removeNumbers)
    corpus <- tm::tm_map(corpus, tm::removeWords, tm::stopwords("english"))
    corpus <- tm::tm_map(corpus, tm::stripWhitespace)
    return(corpus)
}

cleaned_corpus <- cleanCorpus(corpus)
cat("Cleaned corpus size:", {length(cleaned_corpus)})

```

## Create DTM and explore data

```{r dtm}

dtm <- tm::DocumentTermMatrix(cleaned_corpus)
cat("DTM dimensions:", {dim(dtm)})

```

## Check for empty documents

```{r empty docs}

dtm_matrix <- as.matrix(dtm)
row_sums <- Matrix::rowSums(dtm_matrix)
cat("Document term counts:", {row_sums})

```

## Normalize the matrix

```{r normalize matrix}

dtm_normalized <- scale(dtm_matrix)

```

## Perform k-means Clustering with k=3
```{r k-means clustering}

set.seed(123)
kmeans_result <- stats::kmeans(dtm_normalized, centers = 3, nstart = 25)

```

## Create visualization using PCA - Principal Component Analysis

```{r PCA}

pca_result <- stats::prcomp(dtm_normalized)
cluster_plot_data <- data.frame(
    PC1 = pca_result$x[,1],
    PC2 = pca_result$x[,2],
    Cluster = factor(kmeans_result$cluster),
    Category = categories
)

# create scatter plot with enhanced visibility
ggplot2::ggplot(cluster_plot_data, ggplot2::aes(PC1, PC2, color = Category, shape = Cluster)) +
    ggplot2::geom_point(size = 5, alpha = 0.7) + 
    ggplot2::labs(title = "Document Clusters",
                    subtitle = "Sports, Technology, and Food Articles",
                    x = "First Principal Component",
                    y = "Second Principal Component") +
    ggthemes::theme_economist_white() +
    ggplot2::theme(legend.position = "right",
            plot.title = ggplot2::element_text(size = 14, face = "bold"),
            legend.text = ggplot2::element_text(size = 10))

```

## Print verification of cluster assignments

```{r cluster assignments}

print("Cluster assignments by category:")
print(table(Category = categories, Cluster = kmeans_result$cluster))

```

## Print top terms for each cluster

```{r top terms}

print("Top terms per cluster:")
terms <- colnames(dtm_matrix)
centers <- kmeans_result$centers
for(i in 1:3) {
    cat(paste0("\nCluster ", i, " top terms:\n"))
    top_indices <- order(centers[i,], decreasing = TRUE)[1:10]
    print(terms[top_indices])
}

```

# Part 2: Complex Text Clustering in R with a K-Means Algorithm

## Prepare the Data

```{r data prep 2}

austen_books <- janeaustenr::austen_books() %>%
    dplyr::group_by(book) %>%
    dplyr::mutate(
        # Add line numbers within each book
        linenumber = row_number(),
        # Create chapter groups
        chapter = cumsum(stringr::str_detect(text, stringr::regex("^chapter [\\dIVXLC]", ignore_case = TRUE)) | 
            stringr::str_detect(text, stringr::regex("^[\\dIVXLC]+", ignore_case = TRUE)))) %>%
    dplyr::ungroup()

```

## Organize by Chapter Words and Create Document-Term Matrix

```{r dtm 2}

stop_words <- tm::stopwords("english")

by_chapter_word <- austen_books %>%
    dplyr::filter(chapter > 0) %>%
    tidyr::unite(document, book, chapter) %>%
    tidytext::unnest_tokens(word, text)

# remove stop words and count words
word_counts <- by_chapter_word %>%
    dplyr::anti_join(tibble::tibble(word = stop_words), by = "word") %>% # changed anti-join
    dplyr::count(document, word, sort = TRUE) %>%
    dplyr::ungroup()

# create a document-term matrix
chapters_dtm <- word_counts %>%
    tidytext::cast_dtm(document, word, n)

```

## Let’s try to reduce the size of the dtm

```{r dtm dim 2}

dim(chapters_dtm)

# remove sparse terms before converting to matrix
chapters_dtm_reduced <- tm::removeSparseTerms(chapters_dtm, sparse = 0.99)
dim(chapters_dtm_reduced)

# now try the conversion with the reduced matrix
chapters_matrix <- as.matrix(chapters_dtm_reduced)
chapters_normalized <- scale(chapters_matrix)

```

## Determine Optimal Number of clusters with Elbow Method

```{r cluster optimization}

wss <- function(k) {
    stats::kmeans(chapters_normalized, k, nstart = 25)$tot.withinss
}

# Calculate WSS for k=1 to k=10
k.values <- 1:10
wss_values <- purrr::map_dbl(k.values, wss)


# Plot elbow curve
elbow_plot <- tibble::tibble(k = k.values, wss = wss_values) %>%
    ggplot2::ggplot(ggplot2::aes(k, wss)) +
    ggplot2::geom_line() +
    ggplot2::geom_point() +
    ggplot2::labs(title = "Elbow Method for Optimal k",
                    x = "Number of Clusters (k)",
                    y = "Total Within-cluster Sum of Squares") +
    ggthemes::theme_economist_white()

elbow_plot

```

## Perform k-means clustering with k=3

```{r k means clustering}

# k=3 means clustering
set.seed(123) # for reproducibility
kmeans_3 <- stats::kmeans(chapters_normalized, centers = 3, nstart = 25)

# Perform k-means clustering with k=6
set.seed(123)
kmeans_6 <- stats::kmeans(chapters_normalized, centers = 6, nstart = 25)

```

## Create visualization and analysis for both

```{r k means visualization}

# First, let's look at which chapters are assigned to which clusters
cluster_assignments_3 <- word_counts %>%
    tidyr::separate(document, c("book", "chapter"), sep = "_") %>%
    dplyr::distinct(book, chapter) %>%
    dplyr::mutate(cluster = kmeans_3$cluster[as.numeric(factor(paste0(book, chapter, sep = "_")))])

cluster_assignments_6 <- word_counts %>%
    tidyr::separate(document, c("book", "chapter"), sep = "_") %>%
    dplyr::distinct(book, chapter) %>%
    dplyr::mutate(cluster = kmeans_6$cluster[as.numeric(factor(paste(book, chapter, sep = "_")))])

```

## Create heatmaps for both k=3 and k=6
```{r k means heatmaps}

heatmap_3 <- cluster_assignments_3 %>%
    dplyr::count(book, cluster) %>%
    ggplot2::ggplot(ggplot2::aes(factor(cluster), book, fill = n)) +
        ggplot2::geom_tile() +
        ggplot2::scale_fill_gradient(low = "white", high = "steelblue") +
        ggplot2::labs(title = "Cluster Distribution (k=3)",
                        x = "Cluster",
                        y = "Book",
                        fill = "Number of Chapters") +
        ggthemes::theme_economist_white()

heatmap_6 <- cluster_assignments_6 %>%
    dplyr::count(book, cluster) %>%
    ggplot2::ggplot(ggplot2::aes(factor(cluster), book, fill = n)) +
        ggplot2::geom_tile() +
        ggplot2::scale_fill_gradient(low = "white", high = "steelblue") +
        ggplot2::labs(title = "Cluster Distribution (k=6)",
                        x = "Cluster",
                        y = "Book",
                        fill = "Number of Chapters") +
        ggthemes::theme_economist_white()

```

## Print Summary Statistics
```{r summary statistics}

print("Cluster sizes for k=3:")
print(table(cluster_assignments_3$cluster))
print("Cluster composition for k=3:")
print(table(cluster_assignments_3$book, cluster_assignments_3$cluster))
print("Cluster sizes for k=6:")
print(table(cluster_assignments_6$cluster))
print("Cluster composition for k=6:")
print(table(cluster_assignments_6$book, cluster_assignments_6$cluster))

```

## Display Heatmaps for both K Values
```{r heatmaps}

print(heatmap_3)
print(heatmap_6)

```

## Calculate and Display Characteristic Words in Each Cluster (k=3)
```{r characteristic words k=3}

cluster_words_3 <- word_counts %>%
    tidyr::separate(document, c("book", "chapter"), sep = "_") %>%
    dplyr::mutate(cluster = kmeans_3$cluster[as.numeric(factor(paste0(book, chapter, sep = "_")))]) %>%
    dplyr::group_by(cluster, word) %>%
    dplyr::summarise(total = sum(n), .groups = 'drop') %>%
    dplyr::group_by(cluster) %>%
    dplyr::slice_max(total, n = 10) %>% # Get top 10 words for each cluster
    dplyr::arrange(cluster, desc(total))

print("Top words in each cluster (k=3):")
print(cluster_words_3)

```

## Calculate and Display Characteristic Words in Each Cluster (k=6)
```{r characteristic-words-k6}

cluster_words_6 <- word_counts %>%
    tidyr::separate(document, c("book", "chapter"), sep = "_") %>%
    dplyr::mutate(cluster = kmeans_6$cluster[as.numeric(factor(paste(book, chapter, sep = "_")))]) %>%
    dplyr::group_by(cluster, word) %>%
    dplyr::summarise(total = sum(n), .groups = 'drop') %>%
    dplyr::group_by(cluster) %>%
    dplyr::slice_max(total, n = 15) %>% # Get top 15 words for each cluster
    dplyr::arrange(cluster, desc(total))

print("Top words in each cluster (k=6):")
print(cluster_words_6)

```

## Print the k=3 results in a more readable format
```{r}

cluster_words_3 %>%
    dplyr::group_by(cluster) %>%
    dplyr::summarise(words = paste0(word, collapse = ", ")) %>%
    dplyr::mutate(cluster = paste0("Cluster", cluster)) %>%
    print(width = Inf) # Print full width

```

# Part 3: Hierarchical (Tree) Clustering in R

## Load the Customer Complaint’s Dataset and Preprocess
```{r}

complaints <- utils::read.csv("Lab_9/data/complaints.csv", stringsAsFactors = FALSE)
# Text preprocessing function
preprocess_text <- function(text) {
    text %>%
        # Convert to lowercase
        stringr::str_to_lower() %>%
        # Remove special characters and digits
        stringr::str_replace_all("[^[:alpha:][:space:]]", "") %>%
        # Remove extra whitespace
        stringr::str_trim() %>%
        stringr::str_squish()
}
# Apply preprocessing
complaints <- complaints %>%
    dplyr::mutate(processed_narrative = purrr::map_chr(consumer_complaint_narrative,
                                                        ~preprocess_text(as.character(.))))
    
# Remove missing values
complaints <- complaints %>%
    dplyr::filter(!is.na(processed_narrative))

# Create corpus
corpus <- tm::Corpus(tm::VectorSource(complaints$processed_narrative))

```

## Perform Hierarchical Clustering
```{r hierarchical clustering}

# Create document-term matrix
dtm_hierarchy <- tm::DocumentTermMatrix(corpus,
                                        control = list(
                                        weighting = tm::weightTfIdf,
                                        stopwords = TRUE,
                                        removeNumbers = TRUE,
                                        removePunctuation = TRUE,
                                        stemming = TRUE))

# Convert to matrix and remove sparse terms
dtm_matrix_hierarchy <- tm::removeSparseTerms(dtm_hierarchy, sparse = 0.99)
dtm_matrix_hierarchy <- as.matrix(dtm_matrix_hierarchy)

```

## Calculate Distance Matrix and Perform Hierarchical Clustering
```{r dist matrix and clustering}

# Using first 100 documents for visualization purposes
dist_matrix <- stats::dist(dtm_matrix_hierarchy[1:100,], method = "euclidean")

# Perform hierarchical clustering
hc <- stats::hclust(dist_matrix, method = "ward.D2")

```

## Visualize the Dendrogram
```{r dendrogram}

# Basic dendrogram
graphics::plot(hc, main = "Hierarchical Clustering Dendrogram",
                xlab = "Documents", ylab = "Height",
                cex = 0.6, hang = -1)

```

```{r enhance dendrogram}

# Enhanced dendrogram using dendextend
dend <- stats::as.dendrogram(hc)
dend <- dendextend::color_branches(dend, k = 5) # Color branches for 5 clusters

# Plot enhanced dendrogram
graphics::par(mar = c(5,5,3,1))
graphics::plot(dend,
                main = "Colored Dendrogram with 5 Clusters",
                ylab = "Height",
                leaflab = "none")

```

## Cut the Tree into Clusters and Analyze
```{r tree cluster analysis}

# Cut tree into 5 clusters
clusters <- dendextend::cutree(hc, k = 5)
# Add cluster assignments to the original data
complaints_subset <- complaints[1:100,] %>%
    dplyr::mutate(cluster = as.factor(clusters))
# Analyze clusters
cluster_summary <- complaints_subset %>%
    dplyr::group_by(cluster) %>%
    dplyr::summarise(
                n = n(),
                sample_complaints = list(utils::head(processed_narrative, 3))
)

# Print cluster summaries
for(i in 1:nrow(cluster_summary)) {
    cat("\nCluster", i, "\n")
    cat("Number of complaints:", cluster_summary$n[i], "\n")
    cat("Sample complaints:\n")
    print(unlist(cluster_summary$sample_complaints[i]))
    cat("\n")
}

```

```{r silhouette plot}

sil <- cluster::silhouette(clusters, dist_matrix)
graphics::plot(sil, main = "Silhouette Plot")

# Average silhouette width
avg_sil <- mean(sil[,3])
cat("Average silhouette width:", avg_sil)

```

```{r color silhouette plot}

# Plot silhouette analysis using factoextra
factoextra::fviz_silhouette(sil)

```

## Get Top Words for Each Cluster
```{r top words}

# Function to get top words for each cluster
get_top_words <- function(cluster_num, n_words = 10) {
    # Get documents in cluster
    cluster_docs <- complaints_subset %>%
        dplyr::filter(cluster == cluster_num) %>%
        dplyr::pull(processed_narrative)
        # Create DTM for cluster
        cluster_corpus <- tm::Corpus(tm::VectorSource(cluster_docs))
        cluster_dtm <- tm::DocumentTermMatrix(cluster_corpus,
                                                control = list(
                                                        weighting = tm::weightTfIdf,
                                                        stopwords = TRUE)
                                                        )
    # Get term frequencies
    term_freq <- colSums(as.matrix(cluster_dtm))
    top_terms <- sort(term_freq, decreasing = TRUE)[1:n_words]
    return(names(top_terms))
}

# Print top words for each cluster
for(i in 1:5) {
    cat("\nCluster", i, "top words:\n")
    print(get_top_words(i))
}

```

## Create Cluster Visualization Using Factoextra
```{r cluster viz}

# Perform PCA on the distance matrix for visualization
pca <- stats::prcomp(dtm_matrix_hierarchy[1:100,])

# Plot clusters in first two principal components
factoextra::fviz_cluster(list(data = pca$x[,1:2], cluster = clusters),
                                main = "Cluster Plot",
                                ellipse.type = "convex",
                                repel = TRUE,
                                show.clust.cent = TRUE)

```

## Compare Different Distance Metrics
```{r metrics comparison}

# Compare different distance metrics
distance_methods <- c("euclidean", "manhattan", "cosine")
dendlist <- list()
for(method in distance_methods) {
    dist_mat <- proxy::dist(dtm_matrix_hierarchy[1:100,], method = method)
    hc_temp <- stats::hclust(dist_mat, method = "ward.D2")
    dendlist[[method]] <- stats::as.dendrogram(hc_temp)
}
# Compare dendrograms
dend_list <- dendextend::dendlist(dendlist[[1]], dendlist[[2]], dendlist[[3]])
names(dend_list) <- distance_methods
dendextend::tanglegram(dend_list[[1]], dend_list[[2]])

```

## Convert Dendrogram to Plotly for Interactive Visualization
```{r plotly visual}

# Convert dendrogram to plotly
dend_data <- ggdendro::dendro_data(dend)

p <- ggplot2::ggplot(ggdendro::segment(dend_data)) +
                        ggplot2::geom_segment(ggplot2::aes(x = x, y = y, xend = xend, yend = yend)) +
                        ggplot2::labs(title = "Dendrogram Visualization") +
                        ggthemes::theme_economist_white()
                        

plotly::ggplotly(p)

```

# Part 4: Introduction to Topic Modeling
